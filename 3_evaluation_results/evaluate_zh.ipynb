{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese RL Evaluation Pipeline\n",
    "\n",
    "This notebook combines data merging and evaluation functionality for Chinese RL (认同逻辑) analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Merging and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sample data with 483 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_type</th>\n",
       "      <th>text</th>\n",
       "      <th>Golden</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>reply_chengdu_1_61_1</td>\n",
       "      <td>reply</td>\n",
       "      <td>热烈欢迎咱叙北第一带盐人 北门可不差，论三环外，东西三环可不一定有北三环好，再等火北弄好，那...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>reply_chengdu_2_2_5</td>\n",
       "      <td>reply</td>\n",
       "      <td>真的很多，我就是周边的，不管现实还是网上看的很多，他们天天说自己土著然后说我们弯脚杆，碰到过...</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>reply_chengdu_2_2_9</td>\n",
       "      <td>reply</td>\n",
       "      <td>我是都江堰的，其他地方的我不敢说，但是我可是从小被成都口音嘲笑哦，我小时候去亲戚家耍过暑假，...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>reply_chengdu_2_2_11</td>\n",
       "      <td>reply</td>\n",
       "      <td>那我说都江堰话被笑的更多，我以前被介绍了个成都人，然后天天给我说他家以前二环以内的，说他家以...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>reply_chengdu_2_2_13</td>\n",
       "      <td>reply</td>\n",
       "      <td>那证明成都人的确不咋地，本来成都话口音离普通话语区就够偏了，很多怪音，他们还歧视本省的人，无...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            comment_id comment_type  \\\n",
       "0           7  reply_chengdu_1_61_1        reply   \n",
       "1          12   reply_chengdu_2_2_5        reply   \n",
       "2          16   reply_chengdu_2_2_9        reply   \n",
       "3          17  reply_chengdu_2_2_11        reply   \n",
       "4          18  reply_chengdu_2_2_13        reply   \n",
       "\n",
       "                                                text Golden  \n",
       "0  热烈欢迎咱叙北第一带盐人 北门可不差，论三环外，东西三环可不一定有北三环好，再等火北弄好，那...      1  \n",
       "1  真的很多，我就是周边的，不管现实还是网上看的很多，他们天天说自己土著然后说我们弯脚杆，碰到过...    147  \n",
       "2  我是都江堰的，其他地方的我不敢说，但是我可是从小被成都口音嘲笑哦，我小时候去亲戚家耍过暑假，...      4  \n",
       "3  那我说都江堰话被笑的更多，我以前被介绍了个成都人，然后天天给我说他家以前二环以内的，说他家以...     14  \n",
       "4  那证明成都人的确不咋地，本来成都话口音离普通话语区就够偏了，很多怪音，他们还歧视本省的人，无...      4  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the sample data\n",
    "sample = pd.read_csv('../0_data_collection/dataset.csv')\n",
    "print(f\"Loaded sample data with {len(sample)} rows\")\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added model: Qwen3-235B-A22B_few_shot\n",
      "Added model: Qwen3-32B_no_cot\n",
      "Added model: deepseek_zero_shot\n",
      "Added model: gpt41_few_shot\n",
      "Added model: gemma-3-27b-it_zero_shot\n",
      "Added model: deepseek_no_cot\n",
      "Added model: Qwen3-32B_few_shot\n",
      "Added model: Qwen3-32B_zero_shot\n",
      "Added model: gpt41_no_cot\n",
      "Added model: deepseek_few_shot\n",
      "Added model: gemma-3-27b-it_no_cot\n",
      "Added model: gpt41_zero_shot\n",
      "Added model: Qwen3-235B-A22B_zero_shot\n",
      "Added model: gemma-3-27b-it_few_shot\n",
      "Added model: Qwen3-235B-A22B_no_cot\n",
      "\n",
      "Final dataframe shape: (483, 20)\n",
      "Columns: ['Unnamed: 0', 'comment_id', 'comment_type', 'text', 'Golden', 'Qwen3-235B-A22B_few_shot', 'Qwen3-32B_no_cot', 'deepseek_zero_shot', 'gpt41_few_shot', 'gemma-3-27b-it_zero_shot', 'deepseek_no_cot', 'Qwen3-32B_few_shot', 'Qwen3-32B_zero_shot', 'gpt41_no_cot', 'deepseek_few_shot', 'gemma-3-27b-it_no_cot', 'gpt41_zero_shot', 'Qwen3-235B-A22B_zero_shot', 'gemma-3-27b-it_few_shot', 'Qwen3-235B-A22B_no_cot']\n"
     ]
    }
   ],
   "source": [
    "# Merge results from all CSV files in the results directory\n",
    "for file in Path('../2_run_llms/llm_outputs').glob(\"*.csv\"):\n",
    "    if not file.stem.startswith('sample') and not file.stem.endswith('en'):\n",
    "        df = pd.read_csv(file)\n",
    "        model = file.stem\n",
    "        sample[model] = df['RL_Types']\n",
    "        print(f\"Added model: {model}\")\n",
    "\n",
    "print(f\"\\nFinal dataframe shape: {sample.shape}\")\n",
    "print(f\"Columns: {list(sample.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_RL_output(text):\n",
    "    \"\"\"Clean English RL outputs (e.g., 'RL1', 'RL23')\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Preserve placeholder for \"no RLs\" if it's simply \"-\"\n",
    "    if text.strip() == \"-\":\n",
    "        return \"-\"\n",
    "\n",
    "    # Find all occurrences of RL followed by digits (e.g., \"RL1\", \"RL23\")\n",
    "    RL_patterns = re.findall(r'RL\\d+', text)\n",
    "    \n",
    "    # Remove duplicates while preserving order of first appearance\n",
    "    unique_RLs = []\n",
    "    seen = set()\n",
    "    for RL in RL_patterns:\n",
    "        if RL not in seen:\n",
    "            unique_RLs.append(RL)\n",
    "            seen.add(RL)\n",
    "\n",
    "    return \", \".join(unique_RLs)\n",
    "\n",
    "\n",
    "def clean_zh_output(text):\n",
    "    \"\"\"Clean Chinese RL outputs (e.g., '认同逻辑1', '认同逻辑4')\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Preserve placeholder for \"no RLs\" if it's simply \"-\"\n",
    "    if text.strip() == \"-\":\n",
    "        return \"-\"\n",
    "\n",
    "    # Find all occurrences of RL followed by digits (e.g., \"认同逻辑1\", \"认同逻辑23\")\n",
    "    RL_patterns = re.findall(r'认同逻辑\\d+', text)\n",
    "    \n",
    "    # Remove duplicates while preserving order of first appearance\n",
    "    unique_RLs = []\n",
    "    seen = set()\n",
    "    for RL in RL_patterns:\n",
    "        if RL not in seen:\n",
    "            unique_RLs.append(RL)\n",
    "            seen.add(RL)\n",
    "\n",
    "    return \", \".join(unique_RLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model columns to clean: ['Qwen3-235B-A22B_few_shot', 'Qwen3-32B_no_cot', 'deepseek_zero_shot', 'gpt41_few_shot', 'gemma-3-27b-it_zero_shot', 'deepseek_no_cot', 'Qwen3-32B_few_shot', 'Qwen3-32B_zero_shot', 'gpt41_no_cot', 'deepseek_few_shot', 'gemma-3-27b-it_no_cot', 'gpt41_zero_shot', 'Qwen3-235B-A22B_zero_shot', 'gemma-3-27b-it_few_shot', 'Qwen3-235B-A22B_no_cot']\n",
      "Cleaned column: Qwen3-235B-A22B_few_shot\n",
      "Cleaned column: Qwen3-32B_no_cot\n",
      "Cleaned column: deepseek_zero_shot\n",
      "Cleaned column: gpt41_few_shot\n",
      "Cleaned column: gemma-3-27b-it_zero_shot\n",
      "Cleaned column: deepseek_no_cot\n",
      "Cleaned column: Qwen3-32B_few_shot\n",
      "Cleaned column: Qwen3-32B_zero_shot\n",
      "Cleaned column: gpt41_no_cot\n",
      "Cleaned column: deepseek_few_shot\n",
      "Cleaned column: gemma-3-27b-it_no_cot\n",
      "Cleaned column: gpt41_zero_shot\n",
      "Cleaned column: Qwen3-235B-A22B_zero_shot\n",
      "Cleaned column: gemma-3-27b-it_few_shot\n",
      "Cleaned column: Qwen3-235B-A22B_no_cot\n",
      "\n",
      "Saved cleaned data to 'sample_all.csv'\n"
     ]
    }
   ],
   "source": [
    "# Identify model output columns to be cleaned\n",
    "known_non_model_cols = ['Unnamed: 0', 'comment_id', 'comment_type', 'text', 'Golden']\n",
    "model_cols_to_clean = [col for col in sample.columns if col not in known_non_model_cols]\n",
    "\n",
    "print(f\"Model columns to clean: {model_cols_to_clean}\")\n",
    "\n",
    "# Apply the cleaning function to each identified model output column\n",
    "for col_name in model_cols_to_clean:\n",
    "    if col_name in sample.columns:\n",
    "        if col_name.endswith('en'):\n",
    "            sample[col_name] = sample[col_name].apply(clean_RL_output)\n",
    "        else:\n",
    "            sample[col_name] = sample[col_name].apply(clean_zh_output)\n",
    "        print(f\"Cleaned column: {col_name}\")\n",
    "\n",
    "# Save the cleaned data\n",
    "sample.to_csv('sample_all.csv', index=False)\n",
    "print(\"\\nSaved cleaned data to 'sample_all.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotator(value):\n",
    "    \"\"\"Parses the 'Golden' column (e.g., \"147\" means labels 1, 4, 7).\"\"\"\n",
    "    if pd.isna(value) or str(value).strip() == \"-\":\n",
    "        return set()\n",
    "    # Ensure value is treated as a string of digits\n",
    "    return set(map(int, list(str(int(value))))) # int(value) handles potential float like 4.0\n",
    "\n",
    "def parse_RL_category(value):\n",
    "    \"\"\"Parses the 'RL Category' column (e.g., \"认同逻辑1, 认同逻辑4\" or \"认同逻辑1\").\"\"\"\n",
    "    if pd.isna(value) or not str(value).strip():\n",
    "        return set()\n",
    "    \n",
    "    labels = set()\n",
    "    # Split by comma, then process each part\n",
    "    items = str(value).split(',')\n",
    "    for item in items:\n",
    "        item = item.strip() # Remove leading/trailing whitespace\n",
    "        if \"认同逻辑\" in item:\n",
    "            try:\n",
    "                # Extract digits after \"认同逻辑\"\n",
    "                label_num = int(item.split(\"认同逻辑\")[1])\n",
    "                labels.add(label_num)\n",
    "            except ValueError:\n",
    "                # Handle cases where number parsing fails\n",
    "                print(f\"Warning: Could not parse number from item: '{item}' in '{value}'\")\n",
    "    return labels\n",
    "\n",
    "def evaluate_predictions(y_true_parsed_list, y_pred_parsed_list, true_label_source_name, pred_label_source_name):\n",
    "    \"\"\"\n",
    "    Calculates and prints F1 scores and classification report for a given pair of true and predicted labels.\n",
    "    Filters out samples where true labels are empty.\n",
    "    \"\"\"\n",
    "    print(f\"\\n\\n{'='*10} EVALUATING: {pred_label_source_name} (Predictions) vs. {true_label_source_name} (Golden Labels) {'='*10}\")\n",
    "\n",
    "    # Filter out samples where true labels are empty\n",
    "    filtered_pairs = [(true, pred) for true, pred in zip(y_true_parsed_list, y_pred_parsed_list) if true]\n",
    "    \n",
    "    if not filtered_pairs:\n",
    "        print(\"No annotated samples found after filtering. Skipping metrics calculation.\")\n",
    "        return\n",
    "        \n",
    "    y_true_filtered, y_pred_filtered = zip(*filtered_pairs)\n",
    "    \n",
    "    # Determine all unique labels present in this specific pairing\n",
    "    all_labels_in_pair = set()\n",
    "    for labels_set in y_true_filtered:\n",
    "        all_labels_in_pair.update(labels_set)\n",
    "    for labels_set in y_pred_filtered:\n",
    "        all_labels_in_pair.update(labels_set)\n",
    "\n",
    "    if not all_labels_in_pair:\n",
    "        print(\"No labels found for this evaluation pair. Skipping metrics calculation.\")\n",
    "        return\n",
    "\n",
    "    sorted_unique_labels = sorted(list(all_labels_in_pair))\n",
    "\n",
    "    # Binarize labels for this specific pair\n",
    "    mlb = MultiLabelBinarizer(classes=sorted_unique_labels)\n",
    "    y_true_binarized = mlb.fit_transform(y_true_filtered)\n",
    "    y_pred_binarized = mlb.transform(y_pred_filtered) # Use transform for predictions\n",
    "\n",
    "    print(f\"Classes considered for this evaluation: {mlb.classes_}\")\n",
    "    print(f\"Number of samples after filtering non-annotated: {len(y_true_filtered)}\")\n",
    "\n",
    "    # Calculate F1 Scores\n",
    "    print(\"\\n--- F1 Scores ---\")\n",
    "    f1_micro = f1_score(y_true_binarized, y_pred_binarized, average='micro', zero_division=0)\n",
    "    print(f\"F1 Score (micro): {f1_micro:.4f}\")\n",
    "\n",
    "    f1_macro = f1_score(y_true_binarized, y_pred_binarized, average='macro', zero_division=0)\n",
    "    print(f\"F1 Score (macro): {f1_macro:.4f}\")\n",
    "\n",
    "    f1_weighted = f1_score(y_true_binarized, y_pred_binarized, average='weighted', zero_division=0)\n",
    "    print(f\"F1 Score (weighted): {f1_weighted:.4f}\")\n",
    "\n",
    "    f1_samples = f1_score(y_true_binarized, y_pred_binarized, average='samples', zero_division=0)\n",
    "    print(f\"F1 Score (samples): {f1_samples:.4f}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    # Full Classification Report\n",
    "    report_target_names = [f\"RL{label}\" for label in mlb.classes_]\n",
    "\n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    try:\n",
    "        report = classification_report(\n",
    "            y_true_binarized,\n",
    "            y_pred_binarized,\n",
    "            target_names=report_target_names,\n",
    "            zero_division=0\n",
    "        )\n",
    "        print(report)\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not generate classification report: {e}\")\n",
    "        print(\"This can happen if some classes in `target_names` are not present in `y_true_binarized` or `y_pred_binarized` after binarization.\")\n",
    "\n",
    "    precision_micro_overall = precision_score(y_true_binarized, y_pred_binarized, average='micro', zero_division=0)\n",
    "    recall_micro_overall = recall_score(y_true_binarized, y_pred_binarized, average='micro', zero_division=0)\n",
    "    print(f\"\\nOverall Micro Precision: {precision_micro_overall:.4f}\")\n",
    "    print(f\"Overall Micro Recall:    {recall_micro_overall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded evaluation data with 483 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_type</th>\n",
       "      <th>text</th>\n",
       "      <th>Golden</th>\n",
       "      <th>Qwen3-235B-A22B_few_shot</th>\n",
       "      <th>Qwen3-32B_no_cot</th>\n",
       "      <th>deepseek_zero_shot</th>\n",
       "      <th>gpt41_few_shot</th>\n",
       "      <th>gemma-3-27b-it_zero_shot</th>\n",
       "      <th>deepseek_no_cot</th>\n",
       "      <th>Qwen3-32B_few_shot</th>\n",
       "      <th>Qwen3-32B_zero_shot</th>\n",
       "      <th>gpt41_no_cot</th>\n",
       "      <th>deepseek_few_shot</th>\n",
       "      <th>gemma-3-27b-it_no_cot</th>\n",
       "      <th>gpt41_zero_shot</th>\n",
       "      <th>Qwen3-235B-A22B_zero_shot</th>\n",
       "      <th>gemma-3-27b-it_few_shot</th>\n",
       "      <th>Qwen3-235B-A22B_no_cot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>reply_chengdu_1_61_1</td>\n",
       "      <td>reply</td>\n",
       "      <td>热烈欢迎咱叙北第一带盐人 北门可不差，论三环外，东西三环可不一定有北三环好，再等火北弄好，那...</td>\n",
       "      <td>1</td>\n",
       "      <td>认同逻辑1, 认同逻辑5</td>\n",
       "      <td>认同逻辑1</td>\n",
       "      <td>认同逻辑1, 认同逻辑5</td>\n",
       "      <td>认同逻辑1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>认同逻辑1, 认同逻辑5</td>\n",
       "      <td>认同逻辑1</td>\n",
       "      <td>认同逻辑1, 认同逻辑5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>认同逻辑1, 认同逻辑5, 认同逻辑6</td>\n",
       "      <td>认同逻辑1</td>\n",
       "      <td>认同逻辑1</td>\n",
       "      <td>认同逻辑1, 认同逻辑5</td>\n",
       "      <td>认同逻辑1</td>\n",
       "      <td>认同逻辑1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>reply_chengdu_2_2_5</td>\n",
       "      <td>reply</td>\n",
       "      <td>真的很多，我就是周边的，不管现实还是网上看的很多，他们天天说自己土著然后说我们弯脚杆，碰到过...</td>\n",
       "      <td>147</td>\n",
       "      <td>认同逻辑1, 认同逻辑4</td>\n",
       "      <td>认同逻辑1, 认同逻辑4</td>\n",
       "      <td>认同逻辑1, 认同逻辑4</td>\n",
       "      <td>认同逻辑1, 认同逻辑4, 认同逻辑6</td>\n",
       "      <td>认同逻辑1, 认同逻辑4, 认同逻辑6</td>\n",
       "      <td>认同逻辑1, 认同逻辑4, 认同逻辑6</td>\n",
       "      <td>认同逻辑1, 认同逻辑4</td>\n",
       "      <td>认同逻辑1, 认同逻辑3, 认同逻辑4, 认同逻辑6</td>\n",
       "      <td>认同逻辑1, 认同逻辑4</td>\n",
       "      <td>认同逻辑1, 认同逻辑4, 认同逻辑6, 认同逻辑7</td>\n",
       "      <td>认同逻辑1, 认同逻辑4</td>\n",
       "      <td>认同逻辑1, 认同逻辑3, 认同逻辑4</td>\n",
       "      <td>认同逻辑1, 认同逻辑2, 认同逻辑3, 认同逻辑4</td>\n",
       "      <td>认同逻辑1, 认同逻辑4, 认同逻辑6, 认同逻辑3</td>\n",
       "      <td>认同逻辑1, 认同逻辑4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>reply_chengdu_2_2_9</td>\n",
       "      <td>reply</td>\n",
       "      <td>我是都江堰的，其他地方的我不敢说，但是我可是从小被成都口音嘲笑哦，我小时候去亲戚家耍过暑假，...</td>\n",
       "      <td>4</td>\n",
       "      <td>认同逻辑4</td>\n",
       "      <td>认同逻辑4</td>\n",
       "      <td>认同逻辑1, 认同逻辑4</td>\n",
       "      <td>认同逻辑4</td>\n",
       "      <td>认同逻辑4</td>\n",
       "      <td>认同逻辑4</td>\n",
       "      <td>认同逻辑4</td>\n",
       "      <td>认同逻辑4</td>\n",
       "      <td>认同逻辑4</td>\n",
       "      <td>认同逻辑4</td>\n",
       "      <td>认同逻辑4</td>\n",
       "      <td>认同逻辑4</td>\n",
       "      <td>认同逻辑3, 认同逻辑4, 认同逻辑1</td>\n",
       "      <td>认同逻辑4</td>\n",
       "      <td>认同逻辑3, 认同逻辑4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>reply_chengdu_2_2_11</td>\n",
       "      <td>reply</td>\n",
       "      <td>那我说都江堰话被笑的更多，我以前被介绍了个成都人，然后天天给我说他家以前二环以内的，说他家以...</td>\n",
       "      <td>14</td>\n",
       "      <td>认同逻辑1, 认同逻辑4</td>\n",
       "      <td>认同逻辑4, 认同逻辑1</td>\n",
       "      <td>认同逻辑1, 认同逻辑4</td>\n",
       "      <td>认同逻辑1, 认同逻辑4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>认同逻辑1, 认同逻辑4</td>\n",
       "      <td>认同逻辑3, 认同逻辑4</td>\n",
       "      <td>认同逻辑1, 认同逻辑4</td>\n",
       "      <td>认同逻辑1, 认同逻辑4</td>\n",
       "      <td>认同逻辑1, 认同逻辑4, 认同逻辑6</td>\n",
       "      <td>认同逻辑4</td>\n",
       "      <td>认同逻辑1, 认同逻辑4</td>\n",
       "      <td>认同逻辑4, 认同逻辑1, 认同逻辑3</td>\n",
       "      <td>认同逻辑1, 认同逻辑4</td>\n",
       "      <td>认同逻辑1, 认同逻辑4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>reply_chengdu_2_2_13</td>\n",
       "      <td>reply</td>\n",
       "      <td>那证明成都人的确不咋地，本来成都话口音离普通话语区就够偏了，很多怪音，他们还歧视本省的人，无...</td>\n",
       "      <td>4</td>\n",
       "      <td>认同逻辑4</td>\n",
       "      <td>认同逻辑1, 认同逻辑4</td>\n",
       "      <td>认同逻辑4</td>\n",
       "      <td>认同逻辑4</td>\n",
       "      <td>认同逻辑4, 认同逻辑6</td>\n",
       "      <td>认同逻辑4</td>\n",
       "      <td>认同逻辑4</td>\n",
       "      <td>认同逻辑4</td>\n",
       "      <td>认同逻辑4</td>\n",
       "      <td>认同逻辑4</td>\n",
       "      <td>认同逻辑4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>认同逻辑4</td>\n",
       "      <td>认同逻辑4</td>\n",
       "      <td>认同逻辑4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            comment_id comment_type  \\\n",
       "0           7  reply_chengdu_1_61_1        reply   \n",
       "1          12   reply_chengdu_2_2_5        reply   \n",
       "2          16   reply_chengdu_2_2_9        reply   \n",
       "3          17  reply_chengdu_2_2_11        reply   \n",
       "4          18  reply_chengdu_2_2_13        reply   \n",
       "\n",
       "                                                text Golden  \\\n",
       "0  热烈欢迎咱叙北第一带盐人 北门可不差，论三环外，东西三环可不一定有北三环好，再等火北弄好，那...      1   \n",
       "1  真的很多，我就是周边的，不管现实还是网上看的很多，他们天天说自己土著然后说我们弯脚杆，碰到过...    147   \n",
       "2  我是都江堰的，其他地方的我不敢说，但是我可是从小被成都口音嘲笑哦，我小时候去亲戚家耍过暑假，...      4   \n",
       "3  那我说都江堰话被笑的更多，我以前被介绍了个成都人，然后天天给我说他家以前二环以内的，说他家以...     14   \n",
       "4  那证明成都人的确不咋地，本来成都话口音离普通话语区就够偏了，很多怪音，他们还歧视本省的人，无...      4   \n",
       "\n",
       "  Qwen3-235B-A22B_few_shot Qwen3-32B_no_cot deepseek_zero_shot  \\\n",
       "0             认同逻辑1, 认同逻辑5            认同逻辑1       认同逻辑1, 认同逻辑5   \n",
       "1             认同逻辑1, 认同逻辑4     认同逻辑1, 认同逻辑4       认同逻辑1, 认同逻辑4   \n",
       "2                    认同逻辑4            认同逻辑4       认同逻辑1, 认同逻辑4   \n",
       "3             认同逻辑1, 认同逻辑4     认同逻辑4, 认同逻辑1       认同逻辑1, 认同逻辑4   \n",
       "4                    认同逻辑4     认同逻辑1, 认同逻辑4              认同逻辑4   \n",
       "\n",
       "        gpt41_few_shot gemma-3-27b-it_zero_shot      deepseek_no_cot  \\\n",
       "0                认同逻辑1                      NaN         认同逻辑1, 认同逻辑5   \n",
       "1  认同逻辑1, 认同逻辑4, 认同逻辑6      认同逻辑1, 认同逻辑4, 认同逻辑6  认同逻辑1, 认同逻辑4, 认同逻辑6   \n",
       "2                认同逻辑4                    认同逻辑4                认同逻辑4   \n",
       "3         认同逻辑1, 认同逻辑4                      NaN         认同逻辑1, 认同逻辑4   \n",
       "4                认同逻辑4             认同逻辑4, 认同逻辑6                认同逻辑4   \n",
       "\n",
       "  Qwen3-32B_few_shot         Qwen3-32B_zero_shot  gpt41_no_cot  \\\n",
       "0              认同逻辑1                认同逻辑1, 认同逻辑5           NaN   \n",
       "1       认同逻辑1, 认同逻辑4  认同逻辑1, 认同逻辑3, 认同逻辑4, 认同逻辑6  认同逻辑1, 认同逻辑4   \n",
       "2              认同逻辑4                       认同逻辑4         认同逻辑4   \n",
       "3       认同逻辑3, 认同逻辑4                认同逻辑1, 认同逻辑4  认同逻辑1, 认同逻辑4   \n",
       "4              认同逻辑4                       认同逻辑4         认同逻辑4   \n",
       "\n",
       "            deepseek_few_shot gemma-3-27b-it_no_cot      gpt41_zero_shot  \\\n",
       "0         认同逻辑1, 认同逻辑5, 认同逻辑6                 认同逻辑1                认同逻辑1   \n",
       "1  认同逻辑1, 认同逻辑4, 认同逻辑6, 认同逻辑7          认同逻辑1, 认同逻辑4  认同逻辑1, 认同逻辑3, 认同逻辑4   \n",
       "2                       认同逻辑4                 认同逻辑4                认同逻辑4   \n",
       "3         认同逻辑1, 认同逻辑4, 认同逻辑6                 认同逻辑4         认同逻辑1, 认同逻辑4   \n",
       "4                       认同逻辑4                 认同逻辑4                  NaN   \n",
       "\n",
       "    Qwen3-235B-A22B_zero_shot     gemma-3-27b-it_few_shot  \\\n",
       "0                认同逻辑1, 认同逻辑5                       认同逻辑1   \n",
       "1  认同逻辑1, 认同逻辑2, 认同逻辑3, 认同逻辑4  认同逻辑1, 认同逻辑4, 认同逻辑6, 认同逻辑3   \n",
       "2         认同逻辑3, 认同逻辑4, 认同逻辑1                       认同逻辑4   \n",
       "3         认同逻辑4, 认同逻辑1, 认同逻辑3                认同逻辑1, 认同逻辑4   \n",
       "4                       认同逻辑4                       认同逻辑4   \n",
       "\n",
       "  Qwen3-235B-A22B_no_cot  \n",
       "0                  认同逻辑1  \n",
       "1           认同逻辑1, 认同逻辑4  \n",
       "2           认同逻辑3, 认同逻辑4  \n",
       "3           认同逻辑1, 认同逻辑4  \n",
       "4                  认同逻辑4  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the cleaned data for evaluation\n",
    "df = pd.read_csv('sample_all.csv')\n",
    "print(f\"Loaded evaluation data with {len(df)} rows\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 483 golden standard annotations\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating Qwen3-235B-A22B_few_shot\n",
      "\n",
      "\n",
      "========== EVALUATING: Qwen3-235B-A22B_few_shot (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.7901\n",
      "F1 Score (macro): 0.7164\n",
      "F1 Score (weighted): 0.7924\n",
      "F1 Score (samples): 0.8027\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.86      0.83      0.84       157\n",
      "         RL2       0.61      0.92      0.73        37\n",
      "         RL3       0.74      0.84      0.79        75\n",
      "         RL4       0.84      0.88      0.86        92\n",
      "         RL5       0.61      0.77      0.68        35\n",
      "         RL6       0.72      0.84      0.78        50\n",
      "         RL7       0.33      0.33      0.33        15\n",
      "\n",
      "   micro avg       0.75      0.83      0.79       461\n",
      "   macro avg       0.67      0.77      0.72       461\n",
      "weighted avg       0.77      0.83      0.79       461\n",
      " samples avg       0.79      0.86      0.80       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.7549\n",
      "Overall Micro Recall:    0.8286\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating Qwen3-32B_no_cot\n",
      "\n",
      "\n",
      "========== EVALUATING: Qwen3-32B_no_cot (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.7363\n",
      "F1 Score (macro): 0.6081\n",
      "F1 Score (weighted): 0.7253\n",
      "F1 Score (samples): 0.7479\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.72      0.84      0.78       157\n",
      "         RL2       0.55      0.62      0.58        37\n",
      "         RL3       0.78      0.69      0.73        75\n",
      "         RL4       0.93      0.80      0.86        92\n",
      "         RL5       0.62      0.51      0.56        35\n",
      "         RL6       0.77      0.72      0.74        50\n",
      "         RL7       0.00      0.00      0.00        15\n",
      "\n",
      "   micro avg       0.75      0.73      0.74       461\n",
      "   macro avg       0.62      0.60      0.61       461\n",
      "weighted avg       0.73      0.73      0.73       461\n",
      " samples avg       0.78      0.76      0.75       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.7461\n",
      "Overall Micro Recall:    0.7267\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating deepseek_zero_shot\n",
      "\n",
      "\n",
      "========== EVALUATING: deepseek_zero_shot (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.7286\n",
      "F1 Score (macro): 0.6640\n",
      "F1 Score (weighted): 0.7334\n",
      "F1 Score (samples): 0.7469\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.79      0.79      0.79       157\n",
      "         RL2       0.49      0.89      0.63        37\n",
      "         RL3       0.54      0.95      0.69        75\n",
      "         RL4       0.78      0.90      0.83        92\n",
      "         RL5       0.50      0.71      0.59        35\n",
      "         RL6       0.57      0.94      0.71        50\n",
      "         RL7       0.50      0.33      0.40        15\n",
      "\n",
      "   micro avg       0.64      0.84      0.73       461\n",
      "   macro avg       0.60      0.79      0.66       461\n",
      "weighted avg       0.67      0.84      0.73       461\n",
      " samples avg       0.70      0.87      0.75       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.6424\n",
      "Overall Micro Recall:    0.8416\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating gpt41_few_shot\n",
      "\n",
      "\n",
      "========== EVALUATING: gpt41_few_shot (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.7566\n",
      "F1 Score (macro): 0.6827\n",
      "F1 Score (weighted): 0.7537\n",
      "F1 Score (samples): 0.7491\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.87      0.69      0.77       157\n",
      "         RL2       0.58      0.81      0.67        37\n",
      "         RL3       0.78      0.79      0.78        75\n",
      "         RL4       0.86      0.87      0.86        92\n",
      "         RL5       0.64      0.51      0.57        35\n",
      "         RL6       0.68      0.84      0.75        50\n",
      "         RL7       0.57      0.27      0.36        15\n",
      "\n",
      "   micro avg       0.77      0.74      0.76       461\n",
      "   macro avg       0.71      0.68      0.68       461\n",
      "weighted avg       0.78      0.74      0.75       461\n",
      " samples avg       0.77      0.77      0.75       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.7720\n",
      "Overall Micro Recall:    0.7419\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating gemma-3-27b-it_zero_shot\n",
      "\n",
      "\n",
      "========== EVALUATING: gemma-3-27b-it_zero_shot (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.6688\n",
      "F1 Score (macro): 0.5682\n",
      "F1 Score (weighted): 0.6653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (samples): 0.6440\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.75      0.70      0.72       157\n",
      "         RL2       0.52      0.65      0.58        37\n",
      "         RL3       0.61      0.72      0.66        75\n",
      "         RL4       0.84      0.75      0.79        92\n",
      "         RL5       0.56      0.51      0.54        35\n",
      "         RL6       0.52      0.64      0.57        50\n",
      "         RL7       0.33      0.07      0.11        15\n",
      "\n",
      "   micro avg       0.67      0.67      0.67       461\n",
      "   macro avg       0.59      0.58      0.57       461\n",
      "weighted avg       0.67      0.67      0.67       461\n",
      " samples avg       0.63      0.71      0.64       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.6696\n",
      "Overall Micro Recall:    0.6681\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating deepseek_no_cot\n",
      "\n",
      "\n",
      "========== EVALUATING: deepseek_no_cot (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.7325\n",
      "F1 Score (macro): 0.6659\n",
      "F1 Score (weighted): 0.7379\n",
      "F1 Score (samples): 0.7501\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.77      0.82      0.80       157\n",
      "         RL2       0.45      0.92      0.61        37\n",
      "         RL3       0.55      0.96      0.70        75\n",
      "         RL4       0.75      0.91      0.82        92\n",
      "         RL5       0.54      0.80      0.64        35\n",
      "         RL6       0.62      0.90      0.74        50\n",
      "         RL7       0.38      0.33      0.36        15\n",
      "\n",
      "   micro avg       0.64      0.86      0.73       461\n",
      "   macro avg       0.58      0.81      0.67       461\n",
      "weighted avg       0.66      0.86      0.74       461\n",
      " samples avg       0.70      0.89      0.75       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.6372\n",
      "Overall Micro Recall:    0.8612\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating Qwen3-32B_few_shot\n",
      "\n",
      "\n",
      "========== EVALUATING: Qwen3-32B_few_shot (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.7915\n",
      "F1 Score (macro): 0.7175\n",
      "F1 Score (weighted): 0.7918\n",
      "F1 Score (samples): 0.7941\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.85      0.78      0.82       157\n",
      "         RL2       0.58      0.81      0.67        37\n",
      "         RL3       0.77      0.91      0.83        75\n",
      "         RL4       0.89      0.90      0.90        92\n",
      "         RL5       0.59      0.66      0.62        35\n",
      "         RL6       0.72      0.84      0.78        50\n",
      "         RL7       0.50      0.33      0.40        15\n",
      "\n",
      "   micro avg       0.77      0.81      0.79       461\n",
      "   macro avg       0.70      0.75      0.72       461\n",
      "weighted avg       0.78      0.81      0.79       461\n",
      " samples avg       0.79      0.84      0.79       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.7727\n",
      "Overall Micro Recall:    0.8113\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating Qwen3-32B_zero_shot\n",
      "\n",
      "\n",
      "========== EVALUATING: Qwen3-32B_zero_shot (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.7355\n",
      "F1 Score (macro): 0.6349\n",
      "F1 Score (weighted): 0.7330\n",
      "F1 Score (samples): 0.7393\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.77      0.82      0.80       157\n",
      "         RL2       0.52      0.89      0.66        37\n",
      "         RL3       0.64      0.84      0.73        75\n",
      "         RL4       0.89      0.82      0.85        92\n",
      "         RL5       0.51      0.54      0.53        35\n",
      "         RL6       0.66      0.70      0.68        50\n",
      "         RL7       0.40      0.13      0.20        15\n",
      "\n",
      "   micro avg       0.70      0.77      0.74       461\n",
      "   macro avg       0.63      0.68      0.63       461\n",
      "weighted avg       0.71      0.77      0.73       461\n",
      " samples avg       0.73      0.81      0.74       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.7022\n",
      "Overall Micro Recall:    0.7722\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating gpt41_no_cot\n",
      "\n",
      "\n",
      "========== EVALUATING: gpt41_no_cot (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.7544\n",
      "F1 Score (macro): 0.6707\n",
      "F1 Score (weighted): 0.7541\n",
      "F1 Score (samples): 0.7504\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.85      0.77      0.81       157\n",
      "         RL2       0.51      0.84      0.63        37\n",
      "         RL3       0.61      0.92      0.73        75\n",
      "         RL4       0.87      0.87      0.87        92\n",
      "         RL5       0.68      0.60      0.64        35\n",
      "         RL6       0.66      0.78      0.72        50\n",
      "         RL7       0.60      0.20      0.30        15\n",
      "\n",
      "   micro avg       0.72      0.79      0.75       461\n",
      "   macro avg       0.68      0.71      0.67       461\n",
      "weighted avg       0.74      0.79      0.75       461\n",
      " samples avg       0.74      0.81      0.75       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.7222\n",
      "Overall Micro Recall:    0.7896\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating deepseek_few_shot\n",
      "\n",
      "\n",
      "========== EVALUATING: deepseek_few_shot (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.7295\n",
      "F1 Score (macro): 0.6436\n",
      "F1 Score (weighted): 0.7368\n",
      "F1 Score (samples): 0.7230\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.83      0.80      0.81       157\n",
      "         RL2       0.48      0.86      0.62        37\n",
      "         RL3       0.69      0.93      0.80        75\n",
      "         RL4       0.72      0.89      0.80        92\n",
      "         RL5       0.46      0.60      0.52        35\n",
      "         RL6       0.56      0.88      0.68        50\n",
      "         RL7       0.25      0.33      0.29        15\n",
      "\n",
      "   micro avg       0.66      0.82      0.73       461\n",
      "   macro avg       0.57      0.76      0.64       461\n",
      "weighted avg       0.68      0.82      0.74       461\n",
      " samples avg       0.69      0.84      0.72       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.6557\n",
      "Overall Micro Recall:    0.8221\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating gemma-3-27b-it_no_cot\n",
      "\n",
      "\n",
      "========== EVALUATING: gemma-3-27b-it_no_cot (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.7321\n",
      "F1 Score (macro): 0.6407\n",
      "F1 Score (weighted): 0.7305\n",
      "F1 Score (samples): 0.7516\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.72      0.85      0.78       157\n",
      "         RL2       0.50      0.84      0.63        37\n",
      "         RL3       0.61      0.87      0.72        75\n",
      "         RL4       0.88      0.86      0.87        92\n",
      "         RL5       0.68      0.49      0.57        35\n",
      "         RL6       0.56      0.80      0.66        50\n",
      "         RL7       0.38      0.20      0.26        15\n",
      "\n",
      "   micro avg       0.67      0.80      0.73       461\n",
      "   macro avg       0.62      0.70      0.64       461\n",
      "weighted avg       0.69      0.80      0.73       461\n",
      " samples avg       0.73      0.84      0.75       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.6746\n",
      "Overall Micro Recall:    0.8004\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating gpt41_zero_shot\n",
      "\n",
      "\n",
      "========== EVALUATING: gpt41_zero_shot (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.7443\n",
      "F1 Score (macro): 0.6574\n",
      "F1 Score (weighted): 0.7433\n",
      "F1 Score (samples): 0.7417\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.81      0.80      0.81       157\n",
      "         RL2       0.51      0.73      0.60        37\n",
      "         RL3       0.60      0.91      0.72        75\n",
      "         RL4       0.85      0.85      0.85        92\n",
      "         RL5       0.66      0.60      0.63        35\n",
      "         RL6       0.68      0.76      0.72        50\n",
      "         RL7       0.50      0.20      0.29        15\n",
      "\n",
      "   micro avg       0.71      0.78      0.74       461\n",
      "   macro avg       0.66      0.69      0.66       461\n",
      "weighted avg       0.72      0.78      0.74       461\n",
      " samples avg       0.73      0.80      0.74       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.7092\n",
      "Overall Micro Recall:    0.7831\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating Qwen3-235B-A22B_zero_shot\n",
      "\n",
      "\n",
      "========== EVALUATING: Qwen3-235B-A22B_zero_shot (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.6993\n",
      "F1 Score (macro): 0.6424\n",
      "F1 Score (weighted): 0.7041\n",
      "F1 Score (samples): 0.7181\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.69      0.83      0.75       157\n",
      "         RL2       0.51      0.89      0.65        37\n",
      "         RL3       0.48      0.89      0.63        75\n",
      "         RL4       0.75      0.90      0.82        92\n",
      "         RL5       0.42      0.71      0.53        35\n",
      "         RL6       0.60      0.84      0.70        50\n",
      "         RL7       0.56      0.33      0.42        15\n",
      "\n",
      "   micro avg       0.60      0.84      0.70       461\n",
      "   macro avg       0.57      0.77      0.64       461\n",
      "weighted avg       0.62      0.84      0.70       461\n",
      " samples avg       0.67      0.86      0.72       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.6003\n",
      "Overall Micro Recall:    0.8373\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating gemma-3-27b-it_few_shot\n",
      "\n",
      "\n",
      "========== EVALUATING: gemma-3-27b-it_few_shot (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.7813\n",
      "F1 Score (macro): 0.6804\n",
      "F1 Score (weighted): 0.7773\n",
      "F1 Score (samples): 0.7945\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.82      0.86      0.84       157\n",
      "         RL2       0.58      0.89      0.70        37\n",
      "         RL3       0.74      0.85      0.80        75\n",
      "         RL4       0.88      0.89      0.89        92\n",
      "         RL5       0.63      0.49      0.55        35\n",
      "         RL6       0.61      0.86      0.72        50\n",
      "         RL7       0.43      0.20      0.27        15\n",
      "\n",
      "   micro avg       0.75      0.82      0.78       461\n",
      "   macro avg       0.67      0.72      0.68       461\n",
      "weighted avg       0.75      0.82      0.78       461\n",
      " samples avg       0.79      0.85      0.79       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.7480\n",
      "Overall Micro Recall:    0.8178\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating Qwen3-235B-A22B_no_cot\n",
      "\n",
      "\n",
      "========== EVALUATING: Qwen3-235B-A22B_no_cot (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.7224\n",
      "F1 Score (macro): 0.6227\n",
      "F1 Score (weighted): 0.7183\n",
      "F1 Score (samples): 0.7463\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.80      0.73      0.77       157\n",
      "         RL2       0.53      0.84      0.65        37\n",
      "         RL3       0.53      0.93      0.67        75\n",
      "         RL4       0.83      0.90      0.86        92\n",
      "         RL5       0.59      0.69      0.63        35\n",
      "         RL6       0.73      0.60      0.66        50\n",
      "         RL7       0.50      0.07      0.12        15\n",
      "\n",
      "   micro avg       0.68      0.77      0.72       461\n",
      "   macro avg       0.64      0.68      0.62       461\n",
      "weighted avg       0.71      0.77      0.72       461\n",
      " samples avg       0.75      0.80      0.75       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.6821\n",
      "Overall Micro Recall:    0.7679\n",
      "\n",
      "\n",
      "Evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "# Parse the golden standard annotations\n",
    "annotator = df['Golden'].apply(parse_annotator)\n",
    "print(f\"Parsed {len(annotator)} golden standard annotations\")\n",
    "\n",
    "# Evaluate each model\n",
    "for column in df.columns[5:]:\n",
    "    if column.endswith('en'):\n",
    "        continue\n",
    "    print(f\"\\n\\n -----------------\\nEvaluating {column}\")\n",
    "    predictions = df[column].apply(parse_RL_category)\n",
    "\n",
    "    # Evaluate predictions against golden standard\n",
    "    evaluate_predictions(\n",
    "        annotator.tolist(),\n",
    "        predictions.tolist(),\n",
    "        true_label_source_name=\"Golden Standard\",\n",
    "        pred_label_source_name=column\n",
    "    )\n",
    "\n",
    "print(\"\\n\\nEvaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Summary:\n",
      "Total samples: 483\n",
      "Number of models evaluated: 15\n"
     ]
    }
   ],
   "source": [
    "# Display summary statistics\n",
    "print(\"Data Summary:\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Number of models evaluated: {len([col for col in df.columns[5:] if not col.endswith('en')])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_cc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
