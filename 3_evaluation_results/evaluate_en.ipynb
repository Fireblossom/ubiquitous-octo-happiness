{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English RL Evaluation Pipeline\n",
    "\n",
    "This notebook combines data merging and evaluation functionality for English RL (Reasoning Logic) analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Merging and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sample data with 483 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_type</th>\n",
       "      <th>text</th>\n",
       "      <th>Golden</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>reply_chengdu_1_61_1</td>\n",
       "      <td>reply</td>\n",
       "      <td>热烈欢迎咱叙北第一带盐人 北门可不差，论三环外，东西三环可不一定有北三环好，再等火北弄好，那...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>reply_chengdu_2_2_5</td>\n",
       "      <td>reply</td>\n",
       "      <td>真的很多，我就是周边的，不管现实还是网上看的很多，他们天天说自己土著然后说我们弯脚杆，碰到过...</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>reply_chengdu_2_2_9</td>\n",
       "      <td>reply</td>\n",
       "      <td>我是都江堰的，其他地方的我不敢说，但是我可是从小被成都口音嘲笑哦，我小时候去亲戚家耍过暑假，...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>reply_chengdu_2_2_11</td>\n",
       "      <td>reply</td>\n",
       "      <td>那我说都江堰话被笑的更多，我以前被介绍了个成都人，然后天天给我说他家以前二环以内的，说他家以...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>reply_chengdu_2_2_13</td>\n",
       "      <td>reply</td>\n",
       "      <td>那证明成都人的确不咋地，本来成都话口音离普通话语区就够偏了，很多怪音，他们还歧视本省的人，无...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            comment_id comment_type  \\\n",
       "0           7  reply_chengdu_1_61_1        reply   \n",
       "1          12   reply_chengdu_2_2_5        reply   \n",
       "2          16   reply_chengdu_2_2_9        reply   \n",
       "3          17  reply_chengdu_2_2_11        reply   \n",
       "4          18  reply_chengdu_2_2_13        reply   \n",
       "\n",
       "                                                text Golden  \n",
       "0  热烈欢迎咱叙北第一带盐人 北门可不差，论三环外，东西三环可不一定有北三环好，再等火北弄好，那...      1  \n",
       "1  真的很多，我就是周边的，不管现实还是网上看的很多，他们天天说自己土著然后说我们弯脚杆，碰到过...    147  \n",
       "2  我是都江堰的，其他地方的我不敢说，但是我可是从小被成都口音嘲笑哦，我小时候去亲戚家耍过暑假，...      4  \n",
       "3  那我说都江堰话被笑的更多，我以前被介绍了个成都人，然后天天给我说他家以前二环以内的，说他家以...     14  \n",
       "4  那证明成都人的确不咋地，本来成都话口音离普通话语区就够偏了，很多怪音，他们还歧视本省的人，无...      4  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the sample data\n",
    "sample = pd.read_csv('../0_data_collection/dataset.csv')\n",
    "print(f\"Loaded sample data with {len(sample)} rows\")\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added model: gpt41_zero_shot_en\n",
      "Added model: deepseek_few_shot_en\n",
      "Added model: Qwen3-235B-A22B_no_cot_en\n",
      "Added model: gemma-3-27b-it_no_cot_en\n",
      "Added model: Qwen3-235B-A22B_zero_shot_en\n",
      "Added model: Qwen3-32B_few_shot_en\n",
      "Added model: gpt41_few_shot_en\n",
      "Added model: Qwen3-32B_zero_shot_en\n",
      "Added model: gemma-3-27b-it_zero_shot_en\n",
      "Added model: deepseek_zero_shot_en\n",
      "Added model: gemma-3-27b-it_few_shot_en\n",
      "Added model: gpt41_no_cot_en\n",
      "Added model: Qwen3-235B-A22B_few_shot_en\n",
      "Added model: deepseek_no_cot_en\n",
      "Added model: Qwen3-32B_no_cot_en\n",
      "\n",
      "Final dataframe shape: (483, 20)\n",
      "Columns: ['Unnamed: 0', 'comment_id', 'comment_type', 'text', 'Golden', 'gpt41_zero_shot_en', 'deepseek_few_shot_en', 'Qwen3-235B-A22B_no_cot_en', 'gemma-3-27b-it_no_cot_en', 'Qwen3-235B-A22B_zero_shot_en', 'Qwen3-32B_few_shot_en', 'gpt41_few_shot_en', 'Qwen3-32B_zero_shot_en', 'gemma-3-27b-it_zero_shot_en', 'deepseek_zero_shot_en', 'gemma-3-27b-it_few_shot_en', 'gpt41_no_cot_en', 'Qwen3-235B-A22B_few_shot_en', 'deepseek_no_cot_en', 'Qwen3-32B_no_cot_en']\n"
     ]
    }
   ],
   "source": [
    "# Merge results from all CSV files in the results directory\n",
    "for file in Path('../2_run_llms/llm_outputs').glob(\"*_en.csv\"):\n",
    "    if not file.stem.startswith('sample'):\n",
    "        df = pd.read_csv(file)\n",
    "        model = file.stem\n",
    "        sample[model] = df['RL_Types']\n",
    "        print(f\"Added model: {model}\")\n",
    "\n",
    "print(f\"\\nFinal dataframe shape: {sample.shape}\")\n",
    "print(f\"Columns: {list(sample.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_RL_output(text):\n",
    "    \"\"\"Clean English RL outputs (e.g., 'RL1', 'RL23', 'Output: Recognition Logic 1')\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Preserve placeholder for \"no RLs\" if it's simply \"-\"\n",
    "    if text.strip() == \"-\":\n",
    "        return \"-\"\n",
    "\n",
    "    # Handle \"Output: Recognition Logic 1, Recognition Logic 4\" format\n",
    "    if \"Output:\" in text and \"Recognition Logic\" in text:\n",
    "        # Extract the part after \"Output:\"\n",
    "        output_part = text.split(\"Output:\")[1].strip()\n",
    "        # Find all \"Recognition Logic N\" patterns\n",
    "        logic_patterns = re.findall(r'Recognition Logic (\\d+)', output_part)\n",
    "        # Convert to RL format\n",
    "        RL_patterns = [f\"RL{num}\" for num in logic_patterns]\n",
    "    else:\n",
    "        # Find all occurrences of RL followed by digits (e.g., \"RL1\", \"RL23\")\n",
    "        RL_patterns = re.findall(r'RL\\d+', text)\n",
    "    \n",
    "    # Remove duplicates while preserving order of first appearance\n",
    "    unique_RLs = []\n",
    "    seen = set()\n",
    "    for RL in RL_patterns:\n",
    "        if RL not in seen:\n",
    "            unique_RLs.append(RL)\n",
    "            seen.add(RL)\n",
    "\n",
    "    return \", \".join(unique_RLs)\n",
    "\n",
    "\n",
    "def clean_zh_output(text):\n",
    "    \"\"\"Clean Chinese RL outputs (e.g., '认同逻辑1', '认同逻辑4')\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Preserve placeholder for \"no RLs\" if it's simply \"-\"\n",
    "    if text.strip() == \"-\":\n",
    "        return \"-\"\n",
    "\n",
    "    # Find all occurrences of RL followed by digits (e.g., \"认同逻辑1\", \"认同逻辑23\")\n",
    "    RL_patterns = re.findall(r'认同逻辑\\d+', text)\n",
    "    \n",
    "    # Remove duplicates while preserving order of first appearance\n",
    "    unique_RLs = []\n",
    "    seen = set()\n",
    "    for RL in RL_patterns:\n",
    "        if RL not in seen:\n",
    "            unique_RLs.append(RL)\n",
    "            seen.add(RL)\n",
    "\n",
    "    return \", \".join(unique_RLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model columns to clean: ['gpt41_zero_shot_en', 'deepseek_few_shot_en', 'Qwen3-235B-A22B_no_cot_en', 'gemma-3-27b-it_no_cot_en', 'Qwen3-235B-A22B_zero_shot_en', 'Qwen3-32B_few_shot_en', 'gpt41_few_shot_en', 'Qwen3-32B_zero_shot_en', 'gemma-3-27b-it_zero_shot_en', 'deepseek_zero_shot_en', 'gemma-3-27b-it_few_shot_en', 'gpt41_no_cot_en', 'Qwen3-235B-A22B_few_shot_en', 'deepseek_no_cot_en', 'Qwen3-32B_no_cot_en']\n",
      "Cleaned column: gpt41_zero_shot_en\n",
      "Cleaned column: deepseek_few_shot_en\n",
      "Cleaned column: Qwen3-235B-A22B_no_cot_en\n",
      "Cleaned column: gemma-3-27b-it_no_cot_en\n",
      "Cleaned column: Qwen3-235B-A22B_zero_shot_en\n",
      "Cleaned column: Qwen3-32B_few_shot_en\n",
      "Cleaned column: gpt41_few_shot_en\n",
      "Cleaned column: Qwen3-32B_zero_shot_en\n",
      "Cleaned column: gemma-3-27b-it_zero_shot_en\n",
      "Cleaned column: deepseek_zero_shot_en\n",
      "Cleaned column: gemma-3-27b-it_few_shot_en\n",
      "Cleaned column: gpt41_no_cot_en\n",
      "Cleaned column: Qwen3-235B-A22B_few_shot_en\n",
      "Cleaned column: deepseek_no_cot_en\n",
      "Cleaned column: Qwen3-32B_no_cot_en\n"
     ]
    }
   ],
   "source": [
    "# Identify model output columns to be cleaned\n",
    "known_non_model_cols = ['Unnamed: 0', 'comment_id', 'comment_type', 'text', 'Golden']\n",
    "model_cols_to_clean = [col for col in sample.columns if col not in known_non_model_cols]\n",
    "\n",
    "print(f\"Model columns to clean: {model_cols_to_clean}\")\n",
    "\n",
    "# Apply the cleaning function to each identified model output column\n",
    "for col_name in model_cols_to_clean:\n",
    "    if col_name in sample.columns:\n",
    "        if col_name.endswith('en'):\n",
    "            sample[col_name] = sample[col_name].apply(clean_RL_output)\n",
    "        else:\n",
    "            sample[col_name] = sample[col_name].apply(clean_zh_output)\n",
    "        print(f\"Cleaned column: {col_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotator(value):\n",
    "    \"\"\"Parses the 'Golden' column (e.g., \"147\" means labels 1, 4, 7).\"\"\"\n",
    "    if pd.isna(value) or str(value).strip() == \"-\":\n",
    "        return set()\n",
    "    # Ensure value is treated as a string of digits\n",
    "    return set(map(int, list(str(int(value))))) # int(value) handles potential float like 4.0\n",
    "\n",
    "def parse_RL_category_en(value):\n",
    "    \"\"\"Parses the 'RL Category' column for English outputs (e.g., \"RL1, RL4\" or \"RL1\").\"\"\"\n",
    "    if pd.isna(value) or not str(value).strip():\n",
    "        return set()\n",
    "    \n",
    "    labels = set()\n",
    "    value_str = str(value).strip()\n",
    "    \n",
    "    # Handle \"Output: Recognition Logic 1, Recognition Logic 4\" format\n",
    "    if \"Output:\" in value_str and \"Recognition Logic\" in value_str:\n",
    "        # Extract the part after \"Output:\"\n",
    "        output_part = value_str.split(\"Output:\")[1].strip()\n",
    "        # Find all \"Recognition Logic N\" patterns\n",
    "        logic_patterns = re.findall(r'Recognition Logic (\\d+)', output_part)\n",
    "        for logic_num in logic_patterns:\n",
    "            try:\n",
    "                labels.add(int(logic_num))\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Could not parse number from 'Recognition Logic {logic_num}' in '{value}'\")\n",
    "        return labels\n",
    "    \n",
    "    # Handle \"RL1, RL4\" format (existing logic)\n",
    "    items = value_str.split(',')\n",
    "    for item in items:\n",
    "        item = item.strip() # Remove leading/trailing whitespace\n",
    "        if \"RL\" in item:\n",
    "            try:\n",
    "                # Extract digits after \"RL\"\n",
    "                label_num = int(item.split(\"RL\")[1])\n",
    "                labels.add(label_num)\n",
    "            except ValueError:\n",
    "                # Handle cases where number parsing fails\n",
    "                print(f\"Warning: Could not parse number from item: '{item}' in '{value}'\")\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def evaluate_predictions(y_true_parsed_list, y_pred_parsed_list, true_label_source_name, pred_label_source_name):\n",
    "    \"\"\"\n",
    "    Calculates and prints F1 scores and classification report for a given pair of true and predicted labels.\n",
    "    Filters out samples where true labels are empty.\n",
    "    \"\"\"\n",
    "    print(f\"\\n\\n{'='*10} EVALUATING: {pred_label_source_name} (Predictions) vs. {true_label_source_name} (Golden Labels) {'='*10}\")\n",
    "\n",
    "    # Filter out samples where true labels are empty\n",
    "    filtered_pairs = [(true, pred) for true, pred in zip(y_true_parsed_list, y_pred_parsed_list) if true]\n",
    "    \n",
    "    if not filtered_pairs:\n",
    "        print(\"No annotated samples found after filtering. Skipping metrics calculation.\")\n",
    "        return\n",
    "        \n",
    "    y_true_filtered, y_pred_filtered = zip(*filtered_pairs)\n",
    "    \n",
    "    # Determine all unique labels present in this specific pairing\n",
    "    all_labels_in_pair = set()\n",
    "    for labels_set in y_true_filtered:\n",
    "        all_labels_in_pair.update(labels_set)\n",
    "    for labels_set in y_pred_filtered:\n",
    "        all_labels_in_pair.update(labels_set)\n",
    "\n",
    "    if not all_labels_in_pair:\n",
    "        print(\"No labels found for this evaluation pair. Skipping metrics calculation.\")\n",
    "        return\n",
    "\n",
    "    sorted_unique_labels = sorted(list(all_labels_in_pair))\n",
    "\n",
    "    # Binarize labels for this specific pair\n",
    "    mlb = MultiLabelBinarizer(classes=sorted_unique_labels)\n",
    "    y_true_binarized = mlb.fit_transform(y_true_filtered)\n",
    "    y_pred_binarized = mlb.transform(y_pred_filtered) # Use transform for predictions\n",
    "\n",
    "    print(f\"Classes considered for this evaluation: {mlb.classes_}\")\n",
    "    print(f\"Number of samples after filtering non-annotated: {len(y_true_filtered)}\")\n",
    "\n",
    "    # Calculate F1 Scores\n",
    "    print(\"\\n--- F1 Scores ---\")\n",
    "    f1_micro = f1_score(y_true_binarized, y_pred_binarized, average='micro', zero_division=0)\n",
    "    print(f\"F1 Score (micro): {f1_micro:.4f}\")\n",
    "\n",
    "    f1_macro = f1_score(y_true_binarized, y_pred_binarized, average='macro', zero_division=0)\n",
    "    print(f\"F1 Score (macro): {f1_macro:.4f}\")\n",
    "\n",
    "    f1_weighted = f1_score(y_true_binarized, y_pred_binarized, average='weighted', zero_division=0)\n",
    "    print(f\"F1 Score (weighted): {f1_weighted:.4f}\")\n",
    "\n",
    "    f1_samples = f1_score(y_true_binarized, y_pred_binarized, average='samples', zero_division=0)\n",
    "    print(f\"F1 Score (samples): {f1_samples:.4f}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    # Full Classification Report\n",
    "    report_target_names = [f\"RL{label}\" for label in mlb.classes_]\n",
    "\n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    try:\n",
    "        report = classification_report(\n",
    "            y_true_binarized,\n",
    "            y_pred_binarized,\n",
    "            target_names=report_target_names,\n",
    "            zero_division=0\n",
    "        )\n",
    "        print(report)\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not generate classification report: {e}\")\n",
    "        print(\"This can happen if some classes in `target_names` are not present in `y_true_binarized` or `y_pred_binarized` after binarization.\")\n",
    "\n",
    "    precision_micro_overall = precision_score(y_true_binarized, y_pred_binarized, average='micro', zero_division=0)\n",
    "    recall_micro_overall = recall_score(y_true_binarized, y_pred_binarized, average='micro', zero_division=0)\n",
    "    print(f\"\\nOverall Micro Precision: {precision_micro_overall:.4f}\")\n",
    "    print(f\"Overall Micro Recall:    {recall_micro_overall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_type</th>\n",
       "      <th>text</th>\n",
       "      <th>Golden</th>\n",
       "      <th>gpt41_zero_shot_en</th>\n",
       "      <th>deepseek_few_shot_en</th>\n",
       "      <th>Qwen3-235B-A22B_no_cot_en</th>\n",
       "      <th>gemma-3-27b-it_no_cot_en</th>\n",
       "      <th>Qwen3-235B-A22B_zero_shot_en</th>\n",
       "      <th>Qwen3-32B_few_shot_en</th>\n",
       "      <th>gpt41_few_shot_en</th>\n",
       "      <th>Qwen3-32B_zero_shot_en</th>\n",
       "      <th>gemma-3-27b-it_zero_shot_en</th>\n",
       "      <th>deepseek_zero_shot_en</th>\n",
       "      <th>gemma-3-27b-it_few_shot_en</th>\n",
       "      <th>gpt41_no_cot_en</th>\n",
       "      <th>Qwen3-235B-A22B_few_shot_en</th>\n",
       "      <th>deepseek_no_cot_en</th>\n",
       "      <th>Qwen3-32B_no_cot_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>reply_chengdu_1_61_1</td>\n",
       "      <td>reply</td>\n",
       "      <td>热烈欢迎咱叙北第一带盐人 北门可不差，论三环外，东西三环可不一定有北三环好，再等火北弄好，那...</td>\n",
       "      <td>1</td>\n",
       "      <td>RL1, RL5</td>\n",
       "      <td>RL1, RL5</td>\n",
       "      <td>RL1, RL5</td>\n",
       "      <td>RL1, RL5</td>\n",
       "      <td>RL1</td>\n",
       "      <td>RL1</td>\n",
       "      <td>RL1</td>\n",
       "      <td>RL1, RL5</td>\n",
       "      <td>RL1, RL2, RL5</td>\n",
       "      <td>RL1, RL5</td>\n",
       "      <td>RL1</td>\n",
       "      <td>RL1, RL5</td>\n",
       "      <td>RL1</td>\n",
       "      <td>RL1, RL5</td>\n",
       "      <td>RL1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>reply_chengdu_2_2_5</td>\n",
       "      <td>reply</td>\n",
       "      <td>真的很多，我就是周边的，不管现实还是网上看的很多，他们天天说自己土著然后说我们弯脚杆，碰到过...</td>\n",
       "      <td>147</td>\n",
       "      <td>RL1, RL4</td>\n",
       "      <td>RL1, RL4, RL7</td>\n",
       "      <td></td>\n",
       "      <td>RL1, RL2, RL3, RL4, RL6</td>\n",
       "      <td>RL1, RL3, RL4</td>\n",
       "      <td>RL1, RL4, RL3</td>\n",
       "      <td>RL1, RL4, RL6</td>\n",
       "      <td>RL1, RL4</td>\n",
       "      <td>RL1, RL3, RL4, RL6</td>\n",
       "      <td>RL1, RL4</td>\n",
       "      <td>RL1, RL4, RL5, RL6</td>\n",
       "      <td>RL1, RL4</td>\n",
       "      <td>RL1, RL4, RL7</td>\n",
       "      <td>RL1, RL4</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>reply_chengdu_2_2_9</td>\n",
       "      <td>reply</td>\n",
       "      <td>我是都江堰的，其他地方的我不敢说，但是我可是从小被成都口音嘲笑哦，我小时候去亲戚家耍过暑假，...</td>\n",
       "      <td>4</td>\n",
       "      <td>RL4</td>\n",
       "      <td>RL4</td>\n",
       "      <td></td>\n",
       "      <td>RL4</td>\n",
       "      <td>RL4</td>\n",
       "      <td>RL4</td>\n",
       "      <td>RL4</td>\n",
       "      <td>RL1, RL4</td>\n",
       "      <td>RL4</td>\n",
       "      <td>RL4</td>\n",
       "      <td>RL4</td>\n",
       "      <td>RL4</td>\n",
       "      <td>RL4</td>\n",
       "      <td>RL4</td>\n",
       "      <td>RL1, RL4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>reply_chengdu_2_2_11</td>\n",
       "      <td>reply</td>\n",
       "      <td>那我说都江堰话被笑的更多，我以前被介绍了个成都人，然后天天给我说他家以前二环以内的，说他家以...</td>\n",
       "      <td>14</td>\n",
       "      <td>RL1, RL4</td>\n",
       "      <td>RL1, RL3, RL4</td>\n",
       "      <td>RL4, RL3</td>\n",
       "      <td>RL4, RL6</td>\n",
       "      <td>RL1, RL3</td>\n",
       "      <td>RL1, RL4</td>\n",
       "      <td>RL4</td>\n",
       "      <td>RL1, RL4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RL3, RL4</td>\n",
       "      <td>RL3, RL4</td>\n",
       "      <td>RL1, RL4</td>\n",
       "      <td>RL4</td>\n",
       "      <td>RL3, RL4</td>\n",
       "      <td>RL1, RL4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>reply_chengdu_2_2_13</td>\n",
       "      <td>reply</td>\n",
       "      <td>那证明成都人的确不咋地，本来成都话口音离普通话语区就够偏了，很多怪音，他们还歧视本省的人，无...</td>\n",
       "      <td>4</td>\n",
       "      <td>RL4</td>\n",
       "      <td>RL4</td>\n",
       "      <td></td>\n",
       "      <td>RL4, RL1</td>\n",
       "      <td>RL1, RL4</td>\n",
       "      <td>RL4</td>\n",
       "      <td>RL4</td>\n",
       "      <td>RL4</td>\n",
       "      <td>RL1, RL4</td>\n",
       "      <td>RL4</td>\n",
       "      <td>RL4</td>\n",
       "      <td>RL4</td>\n",
       "      <td>RL4</td>\n",
       "      <td>RL4</td>\n",
       "      <td>RL1, RL4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            comment_id comment_type  \\\n",
       "0           7  reply_chengdu_1_61_1        reply   \n",
       "1          12   reply_chengdu_2_2_5        reply   \n",
       "2          16   reply_chengdu_2_2_9        reply   \n",
       "3          17  reply_chengdu_2_2_11        reply   \n",
       "4          18  reply_chengdu_2_2_13        reply   \n",
       "\n",
       "                                                text Golden  \\\n",
       "0  热烈欢迎咱叙北第一带盐人 北门可不差，论三环外，东西三环可不一定有北三环好，再等火北弄好，那...      1   \n",
       "1  真的很多，我就是周边的，不管现实还是网上看的很多，他们天天说自己土著然后说我们弯脚杆，碰到过...    147   \n",
       "2  我是都江堰的，其他地方的我不敢说，但是我可是从小被成都口音嘲笑哦，我小时候去亲戚家耍过暑假，...      4   \n",
       "3  那我说都江堰话被笑的更多，我以前被介绍了个成都人，然后天天给我说他家以前二环以内的，说他家以...     14   \n",
       "4  那证明成都人的确不咋地，本来成都话口音离普通话语区就够偏了，很多怪音，他们还歧视本省的人，无...      4   \n",
       "\n",
       "  gpt41_zero_shot_en deepseek_few_shot_en Qwen3-235B-A22B_no_cot_en  \\\n",
       "0           RL1, RL5             RL1, RL5                  RL1, RL5   \n",
       "1           RL1, RL4        RL1, RL4, RL7                             \n",
       "2                RL4                  RL4                             \n",
       "3           RL1, RL4        RL1, RL3, RL4                  RL4, RL3   \n",
       "4                RL4                  RL4                             \n",
       "\n",
       "  gemma-3-27b-it_no_cot_en Qwen3-235B-A22B_zero_shot_en Qwen3-32B_few_shot_en  \\\n",
       "0                 RL1, RL5                          RL1                   RL1   \n",
       "1  RL1, RL2, RL3, RL4, RL6                RL1, RL3, RL4         RL1, RL4, RL3   \n",
       "2                      RL4                          RL4                   RL4   \n",
       "3                 RL4, RL6                     RL1, RL3              RL1, RL4   \n",
       "4                 RL4, RL1                     RL1, RL4                   RL4   \n",
       "\n",
       "  gpt41_few_shot_en Qwen3-32B_zero_shot_en gemma-3-27b-it_zero_shot_en  \\\n",
       "0               RL1               RL1, RL5               RL1, RL2, RL5   \n",
       "1     RL1, RL4, RL6               RL1, RL4          RL1, RL3, RL4, RL6   \n",
       "2               RL4               RL1, RL4                         RL4   \n",
       "3               RL4               RL1, RL4                         NaN   \n",
       "4               RL4                    RL4                    RL1, RL4   \n",
       "\n",
       "  deepseek_zero_shot_en gemma-3-27b-it_few_shot_en gpt41_no_cot_en  \\\n",
       "0              RL1, RL5                        RL1        RL1, RL5   \n",
       "1              RL1, RL4         RL1, RL4, RL5, RL6        RL1, RL4   \n",
       "2                   RL4                        RL4             RL4   \n",
       "3              RL3, RL4                   RL3, RL4        RL1, RL4   \n",
       "4                   RL4                        RL4             RL4   \n",
       "\n",
       "  Qwen3-235B-A22B_few_shot_en deepseek_no_cot_en Qwen3-32B_no_cot_en  \n",
       "0                         RL1           RL1, RL5                 RL1  \n",
       "1               RL1, RL4, RL7           RL1, RL4                      \n",
       "2                         RL4                RL4            RL1, RL4  \n",
       "3                         RL4           RL3, RL4            RL1, RL4  \n",
       "4                         RL4                RL4            RL1, RL4  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the cleaned data for evaluation\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 483 golden standard annotations\n",
      "Evaluating 15 English models\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating gpt41_zero_shot_en\n",
      "\n",
      "\n",
      "========== EVALUATING: gpt41_zero_shot_en (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.7130\n",
      "F1 Score (macro): 0.6365\n",
      "F1 Score (weighted): 0.7048\n",
      "F1 Score (samples): 0.6570\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.79      0.61      0.69       157\n",
      "         RL2       0.58      0.70      0.63        37\n",
      "         RL3       0.68      0.81      0.74        75\n",
      "         RL4       0.91      0.86      0.88        92\n",
      "         RL5       0.67      0.40      0.50        35\n",
      "         RL6       0.83      0.60      0.70        50\n",
      "         RL7       0.75      0.20      0.32        15\n",
      "\n",
      "   micro avg       0.76      0.67      0.71       461\n",
      "   macro avg       0.74      0.60      0.64       461\n",
      "weighted avg       0.77      0.67      0.70       461\n",
      " samples avg       0.67      0.68      0.66       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.7643\n",
      "Overall Micro Recall:    0.6681\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating deepseek_few_shot_en\n",
      "\n",
      "\n",
      "========== EVALUATING: deepseek_few_shot_en (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.7240\n",
      "F1 Score (macro): 0.6631\n",
      "F1 Score (weighted): 0.7343\n",
      "F1 Score (samples): 0.7100\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.82      0.85      0.83       157\n",
      "         RL2       0.43      0.89      0.58        37\n",
      "         RL3       0.62      0.93      0.75        75\n",
      "         RL4       0.71      0.85      0.77        92\n",
      "         RL5       0.55      0.66      0.60        35\n",
      "         RL6       0.49      0.78      0.60        50\n",
      "         RL7       0.40      0.67      0.50        15\n",
      "\n",
      "   micro avg       0.64      0.84      0.72       461\n",
      "   macro avg       0.58      0.80      0.66       461\n",
      "weighted avg       0.66      0.84      0.73       461\n",
      " samples avg       0.65      0.85      0.71       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.6365\n",
      "Overall Micro Recall:    0.8395\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating Qwen3-235B-A22B_no_cot_en\n",
      "\n",
      "\n",
      "========== EVALUATING: Qwen3-235B-A22B_no_cot_en (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.4315\n",
      "F1 Score (macro): 0.3491\n",
      "F1 Score (weighted): 0.4295\n",
      "F1 Score (samples): 0.2880\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.74      0.36      0.48       157\n",
      "         RL2       0.42      0.38      0.40        37\n",
      "         RL3       0.56      0.40      0.47        75\n",
      "         RL4       0.63      0.39      0.48        92\n",
      "         RL5       0.25      0.11      0.16        35\n",
      "         RL6       0.58      0.38      0.46        50\n",
      "         RL7       0.00      0.00      0.00        15\n",
      "\n",
      "   micro avg       0.58      0.34      0.43       461\n",
      "   macro avg       0.45      0.29      0.35       461\n",
      "weighted avg       0.58      0.34      0.43       461\n",
      " samples avg       0.27      0.35      0.29       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.5761\n",
      "Overall Micro Recall:    0.3449\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating gemma-3-27b-it_no_cot_en\n",
      "\n",
      "\n",
      "========== EVALUATING: gemma-3-27b-it_no_cot_en (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.6726\n",
      "F1 Score (macro): 0.5589\n",
      "F1 Score (weighted): 0.6721\n",
      "F1 Score (samples): 0.6895\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.66      0.87      0.75       157\n",
      "         RL2       0.43      0.89      0.58        37\n",
      "         RL3       0.43      0.84      0.57        75\n",
      "         RL4       0.81      0.88      0.84        92\n",
      "         RL5       0.49      0.63      0.55        35\n",
      "         RL6       0.47      0.88      0.61        50\n",
      "         RL7       0.00      0.00      0.00        15\n",
      "\n",
      "   micro avg       0.57      0.82      0.67       461\n",
      "   macro avg       0.47      0.71      0.56       461\n",
      "weighted avg       0.58      0.82      0.67       461\n",
      " samples avg       0.63      0.85      0.69       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.5680\n",
      "Overall Micro Recall:    0.8243\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating Qwen3-235B-A22B_zero_shot_en\n",
      "\n",
      "\n",
      "========== EVALUATING: Qwen3-235B-A22B_zero_shot_en (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.7155\n",
      "F1 Score (macro): 0.6147\n",
      "F1 Score (weighted): 0.7115\n",
      "F1 Score (samples): 0.7182\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.73      0.85      0.79       157\n",
      "         RL2       0.56      0.68      0.61        37\n",
      "         RL3       0.54      0.92      0.68        75\n",
      "         RL4       0.75      0.87      0.81        92\n",
      "         RL5       0.54      0.57      0.56        35\n",
      "         RL6       0.64      0.74      0.69        50\n",
      "         RL7       0.25      0.13      0.17        15\n",
      "\n",
      "   micro avg       0.65      0.79      0.72       461\n",
      "   macro avg       0.57      0.68      0.61       461\n",
      "weighted avg       0.65      0.79      0.71       461\n",
      " samples avg       0.69      0.82      0.72       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.6512\n",
      "Overall Micro Recall:    0.7939\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating Qwen3-32B_few_shot_en\n",
      "\n",
      "\n",
      "========== EVALUATING: Qwen3-32B_few_shot_en (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.7623\n",
      "F1 Score (macro): 0.6720\n",
      "F1 Score (weighted): 0.7600\n",
      "F1 Score (samples): 0.7708\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.85      0.75      0.79       157\n",
      "         RL2       0.58      0.78      0.67        37\n",
      "         RL3       0.71      0.85      0.78        75\n",
      "         RL4       0.93      0.85      0.89        92\n",
      "         RL5       0.64      0.51      0.57        35\n",
      "         RL6       0.72      0.78      0.75        50\n",
      "         RL7       0.38      0.20      0.26        15\n",
      "\n",
      "   micro avg       0.77      0.75      0.76       461\n",
      "   macro avg       0.69      0.67      0.67       461\n",
      "weighted avg       0.78      0.75      0.76       461\n",
      " samples avg       0.79      0.79      0.77       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.7699\n",
      "Overall Micro Recall:    0.7549\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating gpt41_few_shot_en\n",
      "\n",
      "\n",
      "========== EVALUATING: gpt41_few_shot_en (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.7648\n",
      "F1 Score (macro): 0.6600\n",
      "F1 Score (weighted): 0.7565\n",
      "F1 Score (samples): 0.7521\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.87      0.72      0.79       157\n",
      "         RL2       0.57      0.84      0.68        37\n",
      "         RL3       0.78      0.80      0.79        75\n",
      "         RL4       0.88      0.89      0.89        92\n",
      "         RL5       0.70      0.40      0.51        35\n",
      "         RL6       0.72      0.82      0.77        50\n",
      "         RL7       0.40      0.13      0.20        15\n",
      "\n",
      "   micro avg       0.79      0.74      0.76       461\n",
      "   macro avg       0.70      0.66      0.66       461\n",
      "weighted avg       0.79      0.74      0.76       461\n",
      " samples avg       0.77      0.78      0.75       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.7867\n",
      "Overall Micro Recall:    0.7440\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating Qwen3-32B_zero_shot_en\n",
      "\n",
      "\n",
      "========== EVALUATING: Qwen3-32B_zero_shot_en (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.7273\n",
      "F1 Score (macro): 0.6256\n",
      "F1 Score (weighted): 0.7216\n",
      "F1 Score (samples): 0.7265\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.75      0.75      0.75       157\n",
      "         RL2       0.54      0.84      0.66        37\n",
      "         RL3       0.58      0.93      0.72        75\n",
      "         RL4       0.87      0.87      0.87        92\n",
      "         RL5       0.57      0.60      0.58        35\n",
      "         RL6       0.67      0.70      0.69        50\n",
      "         RL7       0.33      0.07      0.11        15\n",
      "\n",
      "   micro avg       0.69      0.77      0.73       461\n",
      "   macro avg       0.62      0.68      0.63       461\n",
      "weighted avg       0.69      0.77      0.72       461\n",
      " samples avg       0.71      0.80      0.73       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.6873\n",
      "Overall Micro Recall:    0.7722\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating gemma-3-27b-it_zero_shot_en\n",
      "\n",
      "\n",
      "========== EVALUATING: gemma-3-27b-it_zero_shot_en (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.7107\n",
      "F1 Score (macro): 0.6218\n",
      "F1 Score (weighted): 0.7114\n",
      "F1 Score (samples): 0.6928\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.80      0.77      0.78       157\n",
      "         RL2       0.53      0.84      0.65        37\n",
      "         RL3       0.50      0.85      0.63        75\n",
      "         RL4       0.89      0.80      0.85        92\n",
      "         RL5       0.61      0.49      0.54        35\n",
      "         RL6       0.57      0.82      0.67        50\n",
      "         RL7       1.00      0.13      0.24        15\n",
      "\n",
      "   micro avg       0.67      0.76      0.71       461\n",
      "   macro avg       0.70      0.67      0.62       461\n",
      "weighted avg       0.71      0.76      0.71       461\n",
      " samples avg       0.67      0.77      0.69       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.6679\n",
      "Overall Micro Recall:    0.7592\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating deepseek_zero_shot_en\n",
      "\n",
      "\n",
      "========== EVALUATING: deepseek_zero_shot_en (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.6954\n",
      "F1 Score (macro): 0.6310\n",
      "F1 Score (weighted): 0.7064\n",
      "F1 Score (samples): 0.7038\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.82      0.76      0.79       157\n",
      "         RL2       0.51      0.92      0.65        37\n",
      "         RL3       0.40      0.96      0.56        75\n",
      "         RL4       0.70      0.92      0.80        92\n",
      "         RL5       0.54      0.71      0.62        35\n",
      "         RL6       0.66      0.80      0.72        50\n",
      "         RL7       0.43      0.20      0.27        15\n",
      "\n",
      "   micro avg       0.60      0.82      0.70       461\n",
      "   macro avg       0.58      0.75      0.63       461\n",
      "weighted avg       0.65      0.82      0.71       461\n",
      " samples avg       0.64      0.85      0.70       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.6025\n",
      "Overall Micro Recall:    0.8221\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating gemma-3-27b-it_few_shot_en\n",
      "\n",
      "\n",
      "========== EVALUATING: gemma-3-27b-it_few_shot_en (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.7384\n",
      "F1 Score (macro): 0.6529\n",
      "F1 Score (weighted): 0.7359\n",
      "F1 Score (samples): 0.7349\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.83      0.70      0.76       157\n",
      "         RL2       0.57      0.92      0.70        37\n",
      "         RL3       0.85      0.63      0.72        75\n",
      "         RL4       0.92      0.90      0.91        92\n",
      "         RL5       0.57      0.34      0.43        35\n",
      "         RL6       0.64      0.84      0.72        50\n",
      "         RL7       0.31      0.33      0.32        15\n",
      "\n",
      "   micro avg       0.76      0.72      0.74       461\n",
      "   macro avg       0.67      0.67      0.65       461\n",
      "weighted avg       0.77      0.72      0.74       461\n",
      " samples avg       0.76      0.75      0.73       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.7551\n",
      "Overall Micro Recall:    0.7223\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating gpt41_no_cot_en\n",
      "\n",
      "\n",
      "========== EVALUATING: gpt41_no_cot_en (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.7323\n",
      "F1 Score (macro): 0.6131\n",
      "F1 Score (weighted): 0.7217\n",
      "F1 Score (samples): 0.6947\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.83      0.67      0.74       157\n",
      "         RL2       0.56      0.65      0.60        37\n",
      "         RL3       0.70      0.88      0.78        75\n",
      "         RL4       0.91      0.80      0.86        92\n",
      "         RL5       0.71      0.49      0.58        35\n",
      "         RL6       0.81      0.68      0.74        50\n",
      "         RL7       0.00      0.00      0.00        15\n",
      "\n",
      "   micro avg       0.77      0.69      0.73       461\n",
      "   macro avg       0.65      0.60      0.61       461\n",
      "weighted avg       0.76      0.69      0.72       461\n",
      " samples avg       0.71      0.72      0.69       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.7748\n",
      "Overall Micro Recall:    0.6941\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating Qwen3-235B-A22B_few_shot_en\n",
      "\n",
      "\n",
      "========== EVALUATING: Qwen3-235B-A22B_few_shot_en (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.7567\n",
      "F1 Score (macro): 0.6694\n",
      "F1 Score (weighted): 0.7499\n",
      "F1 Score (samples): 0.7471\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.85      0.69      0.76       157\n",
      "         RL2       0.66      0.68      0.67        37\n",
      "         RL3       0.80      0.84      0.82        75\n",
      "         RL4       0.93      0.82      0.87        92\n",
      "         RL5       0.75      0.43      0.55        35\n",
      "         RL6       0.77      0.72      0.74        50\n",
      "         RL7       0.50      0.20      0.29        15\n",
      "\n",
      "   micro avg       0.82      0.70      0.76       461\n",
      "   macro avg       0.75      0.62      0.67       461\n",
      "weighted avg       0.81      0.70      0.75       461\n",
      " samples avg       0.79      0.74      0.75       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.8166\n",
      "Overall Micro Recall:    0.7050\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating deepseek_no_cot_en\n",
      "\n",
      "\n",
      "========== EVALUATING: deepseek_no_cot_en (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.6872\n",
      "F1 Score (macro): 0.6202\n",
      "F1 Score (weighted): 0.6977\n",
      "F1 Score (samples): 0.6916\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.83      0.75      0.79       157\n",
      "         RL2       0.50      0.92      0.65        37\n",
      "         RL3       0.40      0.97      0.57        75\n",
      "         RL4       0.67      0.93      0.78        92\n",
      "         RL5       0.49      0.71      0.58        35\n",
      "         RL6       0.61      0.82      0.70        50\n",
      "         RL7       0.43      0.20      0.27        15\n",
      "\n",
      "   micro avg       0.59      0.82      0.69       461\n",
      "   macro avg       0.56      0.76      0.62       461\n",
      "weighted avg       0.64      0.82      0.70       461\n",
      " samples avg       0.63      0.85      0.69       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.5891\n",
      "Overall Micro Recall:    0.8243\n",
      "\n",
      "\n",
      " -----------------\n",
      "Evaluating Qwen3-32B_no_cot_en\n",
      "\n",
      "\n",
      "========== EVALUATING: Qwen3-32B_no_cot_en (Predictions) vs. Golden Standard (Golden Labels) ==========\n",
      "Classes considered for this evaluation: [1 2 3 4 5 6 7]\n",
      "Number of samples after filtering non-annotated: 343\n",
      "\n",
      "--- F1 Scores ---\n",
      "F1 Score (micro): 0.6827\n",
      "F1 Score (macro): 0.5870\n",
      "F1 Score (weighted): 0.6775\n",
      "F1 Score (samples): 0.6834\n",
      "--------------------\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RL1       0.62      0.81      0.70       157\n",
      "         RL2       0.51      0.59      0.55        37\n",
      "         RL3       0.62      0.71      0.66        75\n",
      "         RL4       0.91      0.74      0.81        92\n",
      "         RL5       0.55      0.51      0.53        35\n",
      "         RL6       0.74      0.74      0.74        50\n",
      "         RL7       0.33      0.07      0.11        15\n",
      "\n",
      "   micro avg       0.66      0.71      0.68       461\n",
      "   macro avg       0.61      0.60      0.59       461\n",
      "weighted avg       0.67      0.71      0.68       461\n",
      " samples avg       0.68      0.74      0.68       461\n",
      "\n",
      "\n",
      "Overall Micro Precision: 0.6599\n",
      "Overall Micro Recall:    0.7072\n",
      "\n",
      "\n",
      "Evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "# Parse the golden standard annotations\n",
    "annotator = sample['Golden'].apply(parse_annotator)\n",
    "print(f\"Parsed {len(annotator)} golden standard annotations\")\n",
    "\n",
    "# Evaluate each English model\n",
    "print(f\"Evaluating {len(sample.columns[5:])} English models\")\n",
    "for column in sample.columns[5:]:\n",
    "    print(f\"\\n\\n -----------------\\nEvaluating {column}\")\n",
    "    predictions = sample[column].apply(parse_RL_category_en)\n",
    "\n",
    "    # Evaluate predictions against golden standard\n",
    "    evaluate_predictions(\n",
    "        annotator.tolist(),\n",
    "        predictions.tolist(),\n",
    "        true_label_source_name=\"Golden Standard\",\n",
    "        pred_label_source_name=column\n",
    "    )\n",
    "\n",
    "print(\"\\n\\nEvaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Summary:\n",
      "Total samples: 483\n",
      "Number of models evaluated: 0\n"
     ]
    }
   ],
   "source": [
    "# Display summary statistics\n",
    "print(\"Data Summary:\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Number of models evaluated: {len([col for col in df.columns[5:] if not col.endswith('en')])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_cc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
